{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89739d1b-6681-4f86-bc86-e345ff134059",
   "metadata": {},
   "source": [
    "## Reflexion\n",
    "\n",
    "> **What you'll learn:** How to build a **Reflexion agent** that iteratively improves its responses through self-critique, external research, and structured revision — using LangGraph, Pydantic, and Tavily Search.\n",
    "\n",
    "The [Reflexion pattern](https://arxiv.org/pdf/2303.11366), introduced by Shinn et al., extends basic reflection by combining **self-critique** with **external knowledge integration** and **structured output parsing**. Unlike simple reflection, Reflexion allows an agent to learn from mistakes in real time while leveraging additional information.\n",
    "\n",
    "The workflow typically follows these steps:\n",
    "- **Initial Generation:** The agent produces a response along with self-critique and research queries.\n",
    "- **External Research:** Knowledge gaps identified during critique trigger web searches or other information retrieval.\n",
    "- **Knowledge Integration:** New insights are incorporated into an improved response.\n",
    "- **Iterative Refinement:** The agent repeats the cycle until the response meets desired quality thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5769d4-2822-41f1-8e76-28766cdeb360",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Import all required libraries: LangChain for LLM orchestration, LangGraph for workflow graphs, Pydantic for structured output validation, and Tavily for web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd29b6ac-62e6-491d-8d63-b6b7d8681f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Import all dependencies for the Reflexion agent\n",
    "# Input   : None\n",
    "# Output  : Loaded modules in namespace\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "# --- LangChain & LangGraph ---\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- Data validation & typing ---\n",
    "from pydantic import ValidationError, BaseModel, Field\n",
    "from typing import Literal, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# --- Utilities ---\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab71bf-894f-4f6f-909f-8c4ea68ac33a",
   "metadata": {},
   "source": [
    "### Define LLM\n",
    "\n",
    "Initialize the language model that will power our Reflexion agent. The factory helpers abstract away API key loading and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76242d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DUPLICATE — alternative LLM configuration, see cell below for active setup]\n",
    "# llm = ChatBedrock(\n",
    "#     model_id=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "#     region_name=\"us-west-2\",\n",
    "#     temperature=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2aeadaa-6b2b-46e8-8816-b13241c9ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM helpers imported successfully!\n",
      "LLM initialized: databricks-claude-opus-4-6 (Databricks)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Import LLM helpers and initialize the language model\n",
    "# Input   : helpers/utils.py factory functions\n",
    "# Output  : `llm` — configured LLM instance\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from helpers.utils import get_groq_llm, get_databricks_llm\n",
    "\n",
    "print(\"LLM helpers imported successfully!\")\n",
    "\n",
    "# --- Select LLM based on platform ---\n",
    "if sys.platform == \"win32\":\n",
    "    llm = get_groq_llm(\"openai/gpt-oss-120b\")\n",
    "elif sys.platform == \"darwin\":\n",
    "    llm = get_databricks_llm(\"databricks-claude-opus-4-6\")\n",
    "else:\n",
    "    print(\"linux\")\n",
    "\n",
    "# --- Confirm initialization ---\n",
    "if hasattr(llm, 'model_name'):\n",
    "    print(f\"LLM initialized: {llm.model_name}\")\n",
    "elif hasattr(llm, 'model'):\n",
    "    print(f\"LLM initialized: {llm.model} (Databricks)\")\n",
    "else:\n",
    "    print(\"LLM initialized: Groq LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e082cd3-a0b0-43a5-96fe-b71908274fe8",
   "metadata": {},
   "source": [
    "### Actor Architecture\n",
    "\n",
    "At its core, a Reflexion agent is built around an **Actor** — an agent that generates an initial response, critiques it, and then re-executes the task with improvements. Supporting this loop are a few critical sub-components:\n",
    "\n",
    "- **Tool execution:** Access to external knowledge sources.\n",
    "- **Initial responder:** Generates the first draft along with self-reflection.\n",
    "- **Revisor/Revision:** Produces refined outputs by incorporating prior reflections.\n",
    "\n",
    "### Construct Tools\n",
    "\n",
    "Since Reflexion requires external knowledge, we first define a tool to fetch information from the web. Here we use `TavilySearchResults`, a wrapper around the Tavily Search API, enabling our agent to perform web searches and gather supporting evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea06c45-4574-496b-baf7-c606ae509772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Initialize the Tavily web search tool\n",
    "# Input   : TAVILY_API_KEY (from environment)\n",
    "# Output  : `tavily_tool` — search tool returning up to 5 results\n",
    "# ─────────────────────────────────────────────\n",
    "# [DUPLICATE imports — TavilySearchResults, TavilySearchAPIWrapper already imported in cell above]\n",
    "\n",
    "web_search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=web_search, max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d9ace",
   "metadata": {},
   "source": [
    "### Define the Prompt Template\n",
    "\n",
    "Next, let's define the prompt that will guide the actor agent's behavior. Prompts serve as the \"role description\" for an agent, specifying what it should and should not do. The agent is instructed to:\n",
    "\n",
    "- Provide an initial explanation.\n",
    "- Reflect and critique its own answer.\n",
    "- Generate search queries to fill knowledge gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Define the actor's system prompt template\n",
    "# Input   : primary_instruction, function_name (partial vars)\n",
    "# Output  : `actor_prompt_template` — reusable prompt for both responder and revisor\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert technical educator specializing in machine learning and neural networks.\n",
    "                Current time: {time}\n",
    "                1. {primary_instruction}\n",
    "                2. Reflect and critique your answer. Be severe to maximize improvement.\n",
    "                3. Recommend search queries to research information and improve your answer.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"\\n\\n<s>Reflect on the user's original question and the\"\n",
    "            \" actions taken thus far. Respond using the {function_name} function.</reminder>\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(\n",
    "    time=lambda: datetime.datetime.now().isoformat(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198f2b9",
   "metadata": {},
   "source": [
    "### Enforce Structured Output\n",
    "\n",
    "When dealing with multi-step workflows, it's always recommended to define structured output models for each sub-agent. To ensure consistency, we define structured outputs using **Pydantic models**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cd8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Define Pydantic schemas for structured LLM output\n",
    "# Input   : None\n",
    "# Output  : `Reflection`, `GenerateResponse` — Pydantic models\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    \"\"\"Generate response. Provide an answer, critique, and then follow up with search queries to improve the answer.\"\"\"\n",
    "    \n",
    "    response: str = Field(description=\"~250 word detailed answer to the question.\")\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
    "    research_queries: list[str] = Field(\n",
    "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34775209",
   "metadata": {},
   "source": [
    "We use Pydantic's `BaseModel` to define two data classes:\n",
    "\n",
    "- **`Reflection`** captures the self-critique, requiring the agent to highlight what information is missing and what is superfluous (unnecessary).\n",
    "- **`GenerateResponse`** structures the final output. It ensures the agent provides its main response, includes a reflection (based on the `Reflection` class), and supplies a list of `research_queries`.\n",
    "\n",
    "This structured approach guarantees that our agents produce consistent and parseable responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24906d",
   "metadata": {},
   "source": [
    "### Add Retry Logic\n",
    "\n",
    "Structured parsing can fail if the output doesn't match the schema. To address this, we add **retry logic with schema feedback** — when validation fails, the error and schema are fed back to the LLM for self-correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b33bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Wrap an LLM chain with retry logic for structured output validation\n",
    "# Input   : chain (LLM pipeline), output_parser (Pydantic parser)\n",
    "# Output  : `AdaptiveResponder` class with `.generate()` method\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "class AdaptiveResponder:\n",
    "    def __init__(self, chain, output_parser):\n",
    "        self.chain = chain\n",
    "        self.output_parser = output_parser\n",
    "    \n",
    "    def generate(self, conversation_state: dict):\n",
    "        llm_response = None\n",
    "        for retry_count in range(3):\n",
    "            llm_response = self.chain.invoke(\n",
    "                {\"messages\": conversation_state[\"messages\"]}, {\"tags\": [f\"attempt:{retry_count}\"]}\n",
    "            )\n",
    "            try:\n",
    "                self.output_parser.invoke(llm_response)\n",
    "                return {\"messages\": llm_response}\n",
    "            except ValidationError as validation_error:\n",
    "                # Fix: Convert schema dict to JSON string\n",
    "                schema_json = json.dumps(self.output_parser.model_json_schema(), indent=2)\n",
    "                conversation_state = conversation_state + [\n",
    "                    llm_response,\n",
    "                    ToolMessage(\n",
    "                        content=f\"{repr(validation_error)}\\n\\nPay close attention to the function schema.\\n\\n{schema_json}\\n\\nRespond by fixing all validation errors.\",\n",
    "                        tool_call_id=llm_response.tool_calls[0][\"id\"],\n",
    "                    ),\n",
    "                ]\n",
    "        return {\"messages\": llm_response}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999bbd8",
   "metadata": {},
   "source": [
    "### Bind the Data Model\n",
    "\n",
    "We now bind the `GenerateResponse` model as a tool. This forces the LLM to output exactly in the defined structure, and we wrap it with `AdaptiveResponder` for retry safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76775a-ecc7-4c49-90f5-64be9380a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Build the initial responder chain with structured output\n",
    "# Input   : actor_prompt_template, llm, GenerateResponse schema\n",
    "# Output  : `initial_responder` — AdaptiveResponder for first-draft generation\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "initial_response_chain = actor_prompt_template.partial(\n",
    "    primary_instruction=\"Provide a detailed ~250 word explanation suitable for someone with basic programming background.\",\n",
    "    function_name=GenerateResponse.__name__,\n",
    ") | llm.bind_tools(tools=[GenerateResponse])\n",
    "\n",
    "response_parser = PydanticToolsParser(tools=[GenerateResponse])\n",
    "\n",
    "initial_responder = AdaptiveResponder(\n",
    "    chain=initial_response_chain, output_parser=response_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df2de7",
   "metadata": {},
   "source": [
    "After invoking `initial_response_chain`, we get a structured output that includes the initial answer, the self-critique, and the generated search queries. Let's test the initial responder with a simple query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82969ab9-ed5a-44b9-a273-552dfe71c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Test the initial responder with a sample question\n",
    "# Input   : example_question (str)\n",
    "# Output  : initial — dict with structured LLM response\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "example_question = \"What is the difference between supervised and unsupervised learning?\"\n",
    "initial = initial_responder.generate(\n",
    "    {\"messages\": [HumanMessage(content=example_question)]}\n",
    ")\n",
    "\n",
    "initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530faa20-2947-4973-9097-236a3052a8ed",
   "metadata": {},
   "source": [
    "### Revision\n",
    "\n",
    "The **Revision** step represents the final stage of the Reflexion loop. Its purpose is to combine three critical elements — the original draft, the self-critique, and the research results — to produce a refined, evidence-backed response.\n",
    "\n",
    "We define a new instruction set (`improvement_guidelines`) that explicitly guides the Revisor:\n",
    "\n",
    "- Integrating critique into the revision process\n",
    "- Adding numerical citations tied to the research evidence\n",
    "- Differentiating correlation from causation in explanations\n",
    "- Including a structured **References** section with clean URLs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Define revision guidelines for the Revisor agent\n",
    "# Input   : None\n",
    "# Output  : `improvement_guidelines` — instruction string\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "improvement_guidelines = \"\"\"Revise your previous explanation using the new information.\n",
    "    - You should use the previous critique to add important technical details to your explanation.\n",
    "    - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
    "    - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit).\n",
    "    - For the references field, provide a clean list of URLs only (e.g., [\"https://example.com\", \"https://example2.com\"])\n",
    "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.\n",
    "    - Keep the explanation accessible for someone with basic programming background while being technically accurate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ddeca",
   "metadata": {},
   "source": [
    "To enforce output structure, we introduce `ImproveResponse` — a Pydantic schema that extends `GenerateResponse` with an additional `sources` field, ensuring that each improved answer comes with verifiable references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43a8a5-8b84-49ed-80f3-a53d154f2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Define the revision output schema with source citations\n",
    "# Input   : Inherits from GenerateResponse\n",
    "# Output  : `ImproveResponse` — Pydantic model with sources field\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "class ImproveResponse(GenerateResponse):\n",
    "    \"\"\"Improve your original answer to your question. Provide an answer, reflection,\n",
    "    cite your reflection with references, and finally\n",
    "    add search queries to improve the answer.\"\"\"\n",
    "    \n",
    "    sources: list[str] = Field(\n",
    "        description=\"List of reference URLs that support your answer. Each reference should be a clean URL string.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7aa1f-f9e3-4e03-a784-2f66804c1973",
   "metadata": {},
   "source": [
    "With the schema defined, we construct the **revision chain** by binding the guidelines to the LLM and wrapping it with `AdaptiveResponder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870d7b1-c476-441b-ba87-5ec7268d80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Build the revision chain with ImproveResponse schema\n",
    "# Input   : actor_prompt_template, llm, improvement_guidelines\n",
    "# Output  : `response_improver` — AdaptiveResponder for revised answers\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "improvement_chain = actor_prompt_template.partial(\n",
    "    primary_instruction=improvement_guidelines,\n",
    "    function_name=ImproveResponse.__name__,\n",
    ") | llm.bind_tools(tools=[ImproveResponse])\n",
    "\n",
    "improvement_parser = PydanticToolsParser(tools=[ImproveResponse])\n",
    "response_improver = AdaptiveResponder(chain=improvement_chain, output_parser=improvement_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811eaec0",
   "metadata": {},
   "source": [
    "We can now test the revision chain by providing a full conversation history — the initial draft, the critique, and the tool output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f18a58-a625-4b22-ab7e-fcd56a2b9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Test the revision chain with initial response + search results\n",
    "# Input   : example_question, initial response, tavily_tool search output\n",
    "# Output  : revised — dict with improved, cited response\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "revised = response_improver.generate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=example_question),\n",
    "            initial[\"messages\"],\n",
    "            ToolMessage(\n",
    "                tool_call_id=initial[\"messages\"].tool_calls[0][\"id\"],\n",
    "                content=json.dumps(\n",
    "                    tavily_tool.invoke(\n",
    "                        {\n",
    "                            \"query\": initial[\"messages\"].tool_calls[0][\"args\"][\n",
    "                                \"research_queries\"\n",
    "                            ][0]\n",
    "                        }\n",
    "                    )\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "revised[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d90e2-3f37-41ab-ad82-50f53ae9f6d0",
   "metadata": {},
   "source": [
    "### Create Tool Node\n",
    "\n",
    "The next step is to execute the tool calls inside a LangGraph workflow. While the Responder and Revisor use different schemas, they both rely on the same external tool (a search API). The key differentiator of Reflexion is its ability to identify knowledge gaps and actively research solutions.\n",
    "\n",
    "The `ToolNode` automatically handles tool execution and result formatting, making it seamless to incorporate external knowledge sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46259dc4-bc28-4b33-9c23-19ffaa349cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Create search tool nodes for use inside the LangGraph workflow\n",
    "# Input   : tavily_tool, GenerateResponse/ImproveResponse schemas\n",
    "# Output  : `search_executor` — ToolNode that runs batch web searches\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "def execute_search_queries(research_queries: list[str], **kwargs):\n",
    "    \"\"\"Execute the generated search queries.\"\"\"\n",
    "    return tavily_tool.batch([{\"query\": search_term} for search_term in research_queries])\n",
    "\n",
    "# Tool node\n",
    "search_executor = ToolNode(\n",
    "    [\n",
    "        StructuredTool.from_function(execute_search_queries, name=GenerateResponse.__name__),\n",
    "        StructuredTool.from_function(execute_search_queries, name=ImproveResponse.__name__),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a422d-7ff3-4999-a4b5-6689f7984dc4",
   "metadata": {},
   "source": [
    "### Building the Graph\n",
    "\n",
    "Finally, we assemble all components — **Responder**, **Tool Executor**, and **Revisor** — into a cyclical graph. This structure captures the iterative nature of Reflexion, where each loop strengthens the final answer.\n",
    "\n",
    "We define the graph state, loop control functions, and wire everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf703dc-94a1-4510-bf33-aed57c32b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Define graph state, loop control, and assemble the Reflexion workflow\n",
    "# Input   : initial_responder, search_executor, response_improver\n",
    "# Output  : `reflexion_workflow` — compiled LangGraph state machine\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "# --- State schema ---\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- Loop control helpers ---\n",
    "def get_iteration_count(message_history: list):\n",
    "    \"\"\"Count recent tool/AI message cycles to track iteration depth.\"\"\"\n",
    "    iteration_count = 0\n",
    "    for message in message_history[::-1]:\n",
    "        if message.type not in {\"tool\", \"ai\"}:\n",
    "            break\n",
    "        iteration_count += 1\n",
    "    return iteration_count\n",
    "\n",
    "def determine_next_action(state: list):\n",
    "    \"\"\"Conditional edge: continue researching or stop after MAXIMUM_CYCLES.\"\"\"\n",
    "    current_iterations = get_iteration_count(state[\"messages\"])\n",
    "    if current_iterations > MAXIMUM_CYCLES:\n",
    "        return END\n",
    "    return \"search_and_research\"\n",
    "\n",
    "# --- Graph construction ---\n",
    "MAXIMUM_CYCLES = 5\n",
    "workflow_builder = StateGraph(State)\n",
    "\n",
    "workflow_builder.add_node(\"create_draft\", initial_responder.generate)\n",
    "workflow_builder.add_node(\"search_and_research\", search_executor)\n",
    "workflow_builder.add_node(\"enhance_response\", response_improver.generate)\n",
    "\n",
    "workflow_builder.add_edge(START, \"create_draft\")\n",
    "workflow_builder.add_edge(\"create_draft\", \"search_and_research\")\n",
    "workflow_builder.add_edge(\"search_and_research\", \"enhance_response\")\n",
    "workflow_builder.add_conditional_edges(\"enhance_response\", determine_next_action, [\"search_and_research\", END])\n",
    "\n",
    "reflexion_workflow = workflow_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123c3da-2a8d-4e5c-9759-1605de84ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Visualize the compiled Reflexion workflow graph\n",
    "# Input   : reflexion_workflow\n",
    "# Output  : Rendered graph diagram\n",
    "# ─────────────────────────────────────────────\n",
    "# [DUPLICATE import — Image, display already imported in cell above]\n",
    "\n",
    "display(Image(reflexion_workflow.get_graph().draw_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfbba2-bb33-48e1-9d12-296d933101db",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Run the complete Reflexion agent end-to-end. The agent will iterate through draft → critique → research → revision cycles until the answer meets quality standards or reaches `MAXIMUM_CYCLES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed997d-a3df-43cc-b297-41bde131868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Run the Reflexion agent end-to-end and display each step\n",
    "# Input   : target_question (str), reflexion_workflow\n",
    "# Output  : Streamed step-by-step agent output\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "target_question = \"How do neural networks actually learn?\"\n",
    "\n",
    "print(f\"Running Reflexion agent with question: {target_question}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "events = reflexion_workflow.stream(\n",
    "    {\"messages\": [(\"user\", target_question)]},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "for i, step in enumerate(events):\n",
    "    print(f\"\\nStep {i}\")\n",
    "    print(\"-\" * 40)\n",
    "    step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Reflexion agent execution completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd2936",
   "metadata": {},
   "source": [
    "### Key Takeaway\n",
    "\n",
    "The Reflexion agent demonstrates a powerful iterative improvement loop:\n",
    "\n",
    "1. **Generate** an initial technical explanation with self-critique\n",
    "2. **Identify** specific knowledge gaps requiring research\n",
    "3. **Execute** targeted web searches for current information\n",
    "4. **Integrate** findings into a comprehensive, cited response\n",
    "5. **Repeat** the process until the explanation meets quality standards\n",
    "\n",
    "By combining structured output (Pydantic), external tool use (Tavily), and cyclic graph execution (LangGraph), Reflexion goes beyond simple reflection — it actively learns and improves in real time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
